# 论文规划：底层控制器与通信约束 (最终版)

## 标题

**Robust and Scalable Multi-Agent Formation Control under Communication Constraints via Graph-based Reinforcement Learning**  
(基于图强化学习的通信受限下鲁棒且可扩展的多智能体编队控制)

## 摘要 (Abstract)

本文研究在通信距离受限、随机丢包等现实约束下，多智能体强化学习编队控制的鲁棒性与可扩展性问题。传统的MASAC等CTDE算法往往假设训练阶段可以获取全局信息，一旦通信受限，其性能与稳定性都会显著下降。为此我们提出**G-MASAC**：在遵守去中心化执行的前提下，将图神经网络嵌入中心化评论家以建模动态通信拓扑，同时在Actor端引入记忆单元处理部分可观测性。仿真表明，G-MASAC在不同通信约束和不同智能体规模下都能保持更低的队形误差和更高的任务成功率，展现出显著的鲁棒性与扩展能力。

## 1. 引言 (Introduction)

- **问题背景**: 多智能体编队在巡检、搜救和协同运输中具有巨大价值，但底层控制器往往忽略通信约束。
- **主要挑战**:
    1. **通信瓶颈**: CTDE范式的中心化评论家依赖全局可用的状态/动作，现实通信半径或丢包会破坏这一假设。
    2. **规模扩展**: 评论家输入随智能体数量线性增长，训练和推理复杂度迅速上升。
- **本文贡献**:
    1. 提出在Critic中集成GNN的G-MASAC结构，显式利用稀疏通信图建模多智能体交互。
    2. 在Actor加入RNN以处理部分可观测性，提升轨迹平滑度与决策稳定性。
    3. 设计通信鲁棒性与可扩展性双重测试，并通过消融实验量化各组件贡献。

## 2. 相关工作 (Related Work)

- **CTDE范式**: 总结MADDPG、MASAC等算法的优势与通信假设，指出其难以扩展到受限通信场景。
- **GNN与MARL结合**: 概述利用图结构捕捉智能体关系的工作，强调我们的Critic采用动态图邻接矩阵并保持Actor去中心化执行的差异点。
- **通信受限控制**: 回顾分布式控制和学习通信策略的研究，为设置通信距离/丢包实验提供依据。

## 3. 方法 (Proposed Method)

### 3.1 问题定义

- **环境**: 在二维平面内的编队跟踪任务，智能体动力学采用离散时间双积分模型，可扩展到仿真器（如MATLAB或Gazebo）中。
- **状态与观测**: 每个智能体的全局状态为`x_i=[p_i, v_i]`，局部观测`o_i`包含本体状态、邻居集合`N_i`的相对位置/速度以及上一控制指令；邻接集合由通信半径`r_c`与丢包指示变量`ζ_{ij}`决定。
- **动作**: 输出二维期望加速度/速度增量`u_i`，经底层控制律`a_i = K_p (p_i^{ref}-p_i) + K_d (v_i^{ref}-v_i) + u_i`执行。
- **奖励函数**: `r = - w_f ||p_i - p_i^{ref}||_2^2 - w_v ||v_i||_2^2 - w_c \sum_{j\neq i}\mathbb{I}(||p_i-p_j||< d_{safe}) - w_u ||u_i||_2^2`，其中`w_f=1.0, w_v=0.1, w_c=5.0, w_u=0.01`。
- **约束**: 速度与控制输入限幅`||u_i|| \le u_{max}`；若通信完全中断超过`0.5s`，则切换到保持阵位模式，由高层发出暂挂信号。

### 3.2 G-MASAC 算法 (核心创新点)

- **回顾MASAC**: 采用SAC的最大熵目标以鼓励探索，并维持温度参数自动调节。
- **Actor网络对部分可观测性的处理**:
    - 输入为长度`k=8`的观测序列，通过单层GRU（隐状态维度`128`）抽取记忆状态`h_t`。
    - 记忆向量与当前观测拼接后送入两层MLP（宽度`256`）输出动作分布的均值与方差，实现平滑决策。
    - 对于异步通信，若缺失观测则使用mask并让GRU维持上一隐状态。
- **基于GNN的评论家网络 (G-Critic)**:
    - 构建以智能体为节点、通信关系为边的图；边权`e_{ij} = ϕ(||p_i-p_j||) · ζ_{ij}`，并在信息缺失时使用插值保持稀疏性。
    - 使用两层GAT（头数`4`，隐状态`128`）聚合邻居的状态-动作，之后通过注意力Pooling获得全局价值估计。
    - **Placement理由**: Critic拥有训练期的拓扑信息补全能力，不增加执行期通信负担。
- **算法流程**:
    1. 收集经验`(o^t, a^t, r^t, o^{t+1}, G^t)`，其中`G^t`为当步通信图。
    2. 使用GNN-Critic最小化时序差分目标，Actor通过策略梯度最大化熵正则化的Q值。
    3. 采用双评论家与目标网络稳定训练，并对通信图进行随机DropEdge以模拟干扰。

### 3.3 训练策略

- **Replay Buffer**: 存储通信图、观测序列和动作，以窗口形式采样。
- **通信增强**: 通过随机裁剪通信边、添加延迟标签来模拟现实网络；延迟`τ`从`U(0,150ms)`抽样并写入`G^t`供Critic学习。
- **正则化**: 在Actor损失中加入动作平滑项`λ_s||u_i(t)-u_i(t-1)||^2`；在Critic中加入图Laplacian正则`λ_L \sum_{(i,j)} ||z_i - z_j||^2`以抑制过拟合。
- **实现细节**: 统一使用Adam优化器、学习率`3e-4`，批量大小`256`，温度参数自动调整。
- **稳定化技巧**: 采用目标网络软更新系数`0.005`，并在训练初期引入模仿高层指令的行为克隆损失。

## 4. 实验与结果 (Experiments and Results)

### 4.1 实验设置

- **对比算法**:
    1. **MASAC-ideal**: 具有全局信息的上限。
    2. **MASAC-partial**: 无GNN，补零缺失邻居信息。
    3. **ISAC**: 独立SAC，作为下限。
- **评价指标**: 平均队形误差、碰撞率、任务成功率、单位时间的通信量、训练收敛速度、能耗；每项指标基于20条随机对齐的测试轨迹并给出95%置信区间。
- **仿真平台**: 使用Gym风格仿真器，动力学时间步`0.1s`，仿真持续`200`步，环境包含随机生成障碍和噪声（`σ_p=0.02 m`, `σ_v=0.01 m/s`）。
- **任务配置**: 目标轨迹由高层规划器提供，初始阵形为楔形，安全距离`d_{safe}=0.5 m`。

### 4.2 实验场景一：通信鲁棒性测试

- **有限通信距离**: 逐步减小通信半径，比较成功率与误差曲线。
- **随机丢包**: 设置0〜40%丢包概率，统计性能与方差，展示G-MASAC的稳定性。
- **通信延迟**: 在链路中注入0–200 ms的随机延迟，测试Critic对时间戳对齐失真的容忍度。

### 4.3 实验场景二：可扩展性测试

- **目标**: 评估算法在3/6/9/12机规模下的性能。
- **方案**: 使用同一训练流程，记录收敛迭代数与最终指标；额外测试将9机模型直接迁移到12机，检验零样本扩展能力。
- **预期**: G-MASAC的性能-规模曲线斜率明显小于对比算法。

### 4.4 消融实验 (Ablation Study)

1. **GNN模块** (`G-MASAC w/o GNN`): 替换为MLP评论家，验证图聚合的重要性。
2. **熵正则** (`G-MASAC w/o Entropy`): 去除最大熵项，观察探索能力下降。
3. **中心化训练** (`G-MASAC w/o Centralized Training`): 仅保留独立Actor，等价ISAC。
4. **记忆模块** (`G-MASAC w/o RNN`): 使用纯MLP Actor，评估时间信息的作用。
5. **通信增强** (`G-MASAC w/o Comm-Aug`): 移除延迟/裁剪增强以验证其对鲁棒性的贡献。

## 5. 结论 (Conclusion)

- **总结**: G-MASAC通过GNN-Critic与记忆Actor同时解决通信鲁棒与可扩展难题，并在各类受限场景中保持稳定编队。
- **未来工作**:
    - 与高层自适应虚拟领航者框架集成，实现端到端系统。
    - 探索学习式通信策略与真实网络仿真，以进一步缩小学术与工程间的差距。
